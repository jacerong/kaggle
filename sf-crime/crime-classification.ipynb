{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Solution Approach to Crime Classification\n",
    "\n",
    "**Author**: Jhon Adrián Cerón-Guzmán.<br>\n",
    "**Date**: March 2020.<br>\n",
    "**Description**: This is my solution approach to the [San Francisco Crime Classification](https://www.kaggle.com/c/sf-crime/) challenge proposed on Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Requirements\n",
    "\n",
    "In order to encourage reproducibility, the following is a list of technologies used, as well as their respective version:\n",
    "\n",
    "1. Python **3.7.2**.\n",
    "2. NumPy **1.17.2**.\n",
    "3. SciPy **1.3.1**.\n",
    "4. Scikit-learn **0.21.3**.\n",
    "5. pandas **0.25.1**.\n",
    "6. Matplotlib **3.1.1**.\n",
    "7. Numba **0.48.0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numba\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_PATH = os.path.abspath(os.getcwd())\n",
    "DATA_PATH = os.path.join(CURRENT_PATH, 'data')\n",
    "\n",
    "DATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "RANDOM_STATE = 91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNAMES = {\n",
    "    'original-train': 'train.csv',\n",
    "    'train': 'projected-train.csv',\n",
    "    'original-test': 'test.csv',\n",
    "    'test': 'projected-test.csv',\n",
    "    'sample-submission': 'sampleSubmission.csv',\n",
    "    'crime-dataset': 'crime-dataset.csv'\n",
    "    }\n",
    "\n",
    "FNAMES = {key: os.path.join(DATA_PATH, fname) for key, fname in FNAMES.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's map each crime category to its numerical representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FNAMES['sample-submission']) as f:\n",
    "    for i, row in enumerate(f):\n",
    "        row = row.rstrip('\\n')\n",
    "        CRIME_CATEGORY = {category: j-1 for j, category in enumerate(row.split(',')) if j > 0}\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECTION = ['Id', 'Dates', 'Category', 'DayOfWeek', 'PdDistrict', 'X', 'Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As specified by the variable `PROJECTION`, let's project (or filter) the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_projection(in_fname, out_fname, projection=PROJECTION):\n",
    "    valid_columns = []\n",
    "    \n",
    "    insert_id = False\n",
    "    header, data = [], []\n",
    "    with open(in_fname) as f:\n",
    "        for i, row in enumerate(f):\n",
    "            row = row.rstrip('\\n')\n",
    "            \n",
    "            if i == 0:\n",
    "                for j, col in enumerate(row.split(',')):\n",
    "                    if col in projection:\n",
    "                        header.append(col)\n",
    "                        valid_columns.append(j)\n",
    "                        \n",
    "                insert_id = True if 'Id' not in header else False\n",
    "\n",
    "                continue\n",
    "                \n",
    "            for old in re.findall(r'\"[^\"]+\"', row):\n",
    "                new = re.sub(r',', '|', old)                \n",
    "                row = row.replace(old, new, 1)\n",
    "                \n",
    "            record = [\n",
    "                re.sub(r'\\|', ',', col).strip()\n",
    "                for j, col in enumerate(row.split(',')) if j in valid_columns\n",
    "                ]\n",
    "            \n",
    "            if len(record) != len(valid_columns):\n",
    "                print('({}), Malformed columns at line {}'.format(in_fname, i+1))\n",
    "                continue\n",
    "            elif insert_id:\n",
    "                record.insert(0, i-1)\n",
    "                \n",
    "            data.append(record)\n",
    "            \n",
    "    if insert_id:\n",
    "        header.insert(0, 'Id')\n",
    "    \n",
    "    return header, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    [FNAMES['original-train'], FNAMES['train']],\n",
    "    [FNAMES['original-test'], FNAMES['test']]\n",
    "    ]\n",
    "for in_fname, out_fname in datasets:\n",
    "    if os.path.isfile(out_fname):\n",
    "        continue\n",
    "    \n",
    "    header, data = dataset_projection(in_fname, out_fname)\n",
    "    df = pd.DataFrame(data, columns=header)\n",
    "    \n",
    "    df['Dates'] = pd.to_datetime(df['Dates'], format=DATE_FORMAT)\n",
    "    df = df.sort_values(by=['Dates'])\n",
    "    df['Dates'] = df['Dates'].dt.strftime(DATE_FORMAT)\n",
    "    \n",
    "    df.to_csv(out_fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's append the test set to the training one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(FNAMES['crime-dataset']):\n",
    "    columns = copy.deepcopy(PROJECTION)\n",
    "    columns.remove('Category')\n",
    "    columns.insert(0, 'Dataset')\n",
    "    \n",
    "    df = None\n",
    "    for in_fname, dataset_type in [[FNAMES['train'], 'train'], [FNAMES['test'], 'test']]:\n",
    "        dataset = pd.read_csv(in_fname)\n",
    "        dataset['Dataset'] = dataset_type\n",
    "        dataset = dataset[columns]\n",
    "        \n",
    "        df = dataset.copy(deep=True) if df is None else df.append(dataset)\n",
    "        \n",
    "    df['Dates'] = pd.to_datetime(df['Dates'], format=DATE_FORMAT)\n",
    "    df = df.sort_values(by=['Dates', 'Dataset', 'Id'])\n",
    "    df['Dates'] = df['Dates'].dt.strftime(DATE_FORMAT)\n",
    "    \n",
    "    df = df[columns]\n",
    "    \n",
    "    df.to_csv(FNAMES['crime-dataset'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FNAMES['crime-dataset'])\n",
    "\n",
    "df['Dates'] = pd.to_datetime(df['Dates'], format=DATE_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TO_IDX = {dataset: i for i, dataset in enumerate(df['Dataset'].unique())}\n",
    "IDX_TO_DATASET = {i: dataset for dataset, i in DATASET_TO_IDX.items()}\n",
    "\n",
    "df['Dataset'] = df['Dataset'].map(DATASET_TO_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTRICT_TO_IDX = {district: i for i, district in enumerate(df['PdDistrict'].unique())}\n",
    "IDX_TO_DISTRICT = {i: district for district, i in DISTRICT_TO_IDX.items()}\n",
    "\n",
    "df['PdDistrict'] = df['PdDistrict'].map(DISTRICT_TO_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECTION = (['Dataset']\n",
    "              + [col for col in PROJECTION if col not in ['Category', 'DayOfWeek']])\n",
    "\n",
    "df = df[PROJECTION]\n",
    "df = df.sort_values(by=['Dates', 'Dataset', 'Id'])\n",
    "\n",
    "crimes = df.copy(deep=True)\n",
    "crimes['ts'] = crimes['Dates'].values.astype(np.int64) // 10 ** 9\n",
    "crimes = crimes[[('ts' if col == 'Dates' else col) for col in PROJECTION]].to_numpy().astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_WINDOWS = [12, 24, 72, 168, 336]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def compute_distance(\n",
    "        lat_1, lon_1,\n",
    "        lat_2, lon_2):\n",
    "    \"\"\"Compute distance between two locations.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Distance in KM.\n",
    "    \n",
    "    Source: <https://stackoverflow.com/questions/19412462/>\n",
    "    \"\"\"\n",
    "    # Approximate radius of earth in KM\n",
    "    earth_radius = 6373.0\n",
    "    \n",
    "    lat_1 = np.radians(lat_1)\n",
    "    lon_1 = np.radians(lon_1)\n",
    "    \n",
    "    lat_2 = np.radians(lat_2)\n",
    "    lon_2 = np.radians(lon_2)\n",
    "    \n",
    "    lon_dist = lon_2 - lon_1\n",
    "    lat_dist = lat_2 - lat_1\n",
    "    \n",
    "    a = (np.square(np.sin(lat_dist/2))\n",
    "         + np.cos(lat_1)\n",
    "         * np.cos(lat_2)\n",
    "         * np.square(np.sin(lon_dist/2)))\n",
    "    \n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    \n",
    "    return earth_radius * c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def compute_aggregated_features(data, time_window, crime_radius):\n",
    "    \"\"\"Compute aggregated features.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray, dtype('int64')\n",
    "        A Numpy-like array of shape \"(n, m)\", where \"n\" is the number\n",
    "        of records and \"m\" is the number of columns (or attributes).\n",
    "        The strict order of the columns is presented below:\n",
    "            Dataset,\n",
    "            Id,\n",
    "            Dates,\n",
    "            PdDistrict,\n",
    "            X - Longitude,\n",
    "            Y - Latitude\n",
    "    time_window : int\n",
    "        Time window (in hours).\n",
    "    crime_radius : list\n",
    "        List of integers, each of which representing a radius in kilometers.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    \n",
    "    # Let's transform the time window into seconds\n",
    "    time_window = time_window * 60 * 60\n",
    "    \n",
    "    aggregated_features = []\n",
    "    for i in range(10000):\n",
    "        ts = data[i,2]    \n",
    "        \n",
    "        lower_ts = ts - time_window\n",
    "        \n",
    "        mask = ((lower_ts < data[:,2])\n",
    "                & (data[:,2] < ts))\n",
    "        \n",
    "        historical_data = data[mask]\n",
    "        m = len(historical_data)\n",
    "        \n",
    "        police_district = data[i,3]\n",
    "        \n",
    "        feature_vector = [\n",
    "            int(data[i,0]),\n",
    "            int(data[i,1]),\n",
    "            m, # number of crimes within the time window\n",
    "            0 # number of crimes attended by the same police department district\n",
    "            ]\n",
    "        feature_vector = feature_vector + [0 for j in crime_radius]\n",
    "        \n",
    "        lat_1 = data[i,5]\n",
    "        lon_1 = data[i,4]\n",
    "        \n",
    "        for j in range(m):\n",
    "            feature_vector[3] += 1 if police_district == historical_data[j,3] else 0\n",
    "            \n",
    "            lat_2 = historical_data[j,5]\n",
    "            lon_2 = historical_data[j,4]\n",
    "            \n",
    "            # Let's compute the number of crimes within each given radius\n",
    "            distance = compute_distance(lat_1, lon_1, lat_2, lon_2)\n",
    "            \n",
    "            for k, rad in enumerate(crime_radius):\n",
    "                feature_vector[4+k] += 1 if distance <= rad else 0\n",
    "                \n",
    "        aggregated_features.append(feature_vector)\n",
    "        \n",
    "    return aggregated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(\n",
    "        df, data, time_windows,\n",
    "        idx_to_dataset, idx_to_district,\n",
    "        crime_radius=[1, 2, 4, 8, 16]):\n",
    "    \"\"\"Compute the process of feature engineering.\"\"\"\n",
    "    agg_ds_fname = os.path.join(DATA_PATH, 'agg-dataset-{}Hrs.csv')\n",
    "    cum_ds_fname = os.path.join(DATA_PATH, 'agg-dataset-{}Hrs-cumulative.csv')\n",
    "    \n",
    "    crime_radius = np.array(crime_radius, dtype=int)\n",
    "    \n",
    "    for i, time_window in enumerate(time_windows):\n",
    "        compute_aggregated_features(data, time_window, crime_radius)\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering(df, crimes, TIME_WINDOWS, IDX_TO_DATASET, IDX_TO_DISTRICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
