{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Solution Approach to Crime Classification\n",
    "\n",
    "**Author**: Jhon Adrián Cerón-Guzmán.<br>\n",
    "**Date**: March 2020.<br>\n",
    "**Description**: This is my solution approach to the [San Francisco Crime Classification](https://www.kaggle.com/c/sf-crime/) challenge proposed on Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Requirements\n",
    "\n",
    "In order to encourage reproducibility, the following is a list of technologies used, as well as their respective version:\n",
    "\n",
    "1. Python **3.7.2**.\n",
    "2. NumPy **1.17.2**.\n",
    "3. SciPy **1.3.1**.\n",
    "4. Scikit-learn **0.21.3**.\n",
    "5. pandas **0.25.1**.\n",
    "6. Matplotlib **3.1.1**.\n",
    "7. Numba **0.48.0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_PATH = os.path.abspath(os.getcwd())\n",
    "DATA_PATH = os.path.join(CURRENT_PATH, 'data')\n",
    "\n",
    "DATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "RANDOM_STATE = 91\n",
    "\n",
    "ALGORITHMS = {\n",
    "    'logit': {\n",
    "        'estimator': LogisticRegression(),\n",
    "        'predict_proba': True,\n",
    "        'param_grid': {\n",
    "            'solver': ['lbfgs'],\n",
    "            'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "            }\n",
    "        },\n",
    "    'gb': {\n",
    "        'estimator': GradientBoostingClassifier(),\n",
    "        'predict_proba': True,\n",
    "        'param_grid': None\n",
    "        },\n",
    "    'rf': {\n",
    "        'estimator': RandomForestClassifier(),\n",
    "        'predict_proba': True,\n",
    "        'param_grid': None\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNAMES = {\n",
    "    'original-train': 'train.csv',\n",
    "    'train': 'projected-train.csv',\n",
    "    'original-test': 'test.csv',\n",
    "    'test': 'projected-test.csv',\n",
    "    'sample-submission': 'sampleSubmission.csv',\n",
    "    'crime-dataset': 'crime-dataset.csv',\n",
    "    'baseline': 'baseline-models.csv'\n",
    "    }\n",
    "\n",
    "FNAMES = {key: os.path.join(DATA_PATH, fname) for key, fname in FNAMES.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's map each crime category to its numerical representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FNAMES['sample-submission']) as f:\n",
    "    for i, row in enumerate(f):\n",
    "        row = row.rstrip('\\n')\n",
    "        CRIME_CATEGORY = {category: j-1 for j, category in enumerate(row.split(',')) if j > 0}\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECTION = ['Id', 'Dates', 'Category', 'DayOfWeek', 'PdDistrict', 'X', 'Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As specified by the variable `PROJECTION`, let's project (or filter) the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_projection(in_fname, out_fname, projection=PROJECTION):\n",
    "    valid_columns = []\n",
    "    \n",
    "    insert_id = False\n",
    "    header, data = [], []\n",
    "    with open(in_fname) as f:\n",
    "        for i, row in enumerate(f):\n",
    "            row = row.rstrip('\\n')\n",
    "            \n",
    "            if i == 0:\n",
    "                for j, col in enumerate(row.split(',')):\n",
    "                    if col in projection:\n",
    "                        header.append(col)\n",
    "                        valid_columns.append(j)\n",
    "                        \n",
    "                insert_id = True if 'Id' not in header else False\n",
    "\n",
    "                continue\n",
    "                \n",
    "            for old in re.findall(r'\"[^\"]+\"', row):\n",
    "                new = re.sub(r',', '|', old)                \n",
    "                row = row.replace(old, new, 1)\n",
    "                \n",
    "            record = [\n",
    "                re.sub(r'\\|', ',', col).strip()\n",
    "                for j, col in enumerate(row.split(',')) if j in valid_columns\n",
    "                ]\n",
    "            \n",
    "            if len(record) != len(valid_columns):\n",
    "                print('({}), Malformed columns at line {}'.format(in_fname, i+1))\n",
    "                continue\n",
    "            elif insert_id:\n",
    "                record.insert(0, i-1)\n",
    "                \n",
    "            data.append(record)\n",
    "            \n",
    "    if insert_id:\n",
    "        header.insert(0, 'Id')\n",
    "    \n",
    "    return header, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    [FNAMES['original-train'], FNAMES['train']],\n",
    "    [FNAMES['original-test'], FNAMES['test']]\n",
    "    ]\n",
    "for in_fname, out_fname in datasets:\n",
    "    if os.path.isfile(out_fname):\n",
    "        continue\n",
    "    \n",
    "    header, data = dataset_projection(in_fname, out_fname)\n",
    "    df = pd.DataFrame(data, columns=header)\n",
    "    \n",
    "    df['Dates'] = pd.to_datetime(df['Dates'], format=DATE_FORMAT)\n",
    "    df = df.sort_values(by=['Dates'])\n",
    "    df['Dates'] = df['Dates'].dt.strftime(DATE_FORMAT)\n",
    "    \n",
    "    df.to_csv(out_fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's append the test set to the training one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(FNAMES['crime-dataset']):\n",
    "    columns = copy.deepcopy(PROJECTION)\n",
    "    columns.remove('Category')\n",
    "    columns.insert(0, 'Dataset')\n",
    "    \n",
    "    df = None\n",
    "    for in_fname, dataset_type in [[FNAMES['train'], 'train'], [FNAMES['test'], 'test']]:\n",
    "        dataset = pd.read_csv(in_fname)\n",
    "        dataset['Dataset'] = dataset_type\n",
    "        dataset = dataset[columns]\n",
    "        \n",
    "        df = dataset.copy(deep=True) if df is None else df.append(dataset)\n",
    "        \n",
    "    df['Dates'] = pd.to_datetime(df['Dates'], format=DATE_FORMAT)\n",
    "    df = df.sort_values(by=['Dates', 'Dataset', 'Id'])\n",
    "    df['Dates'] = df['Dates'].dt.strftime(DATE_FORMAT)\n",
    "    \n",
    "    df = df[columns]\n",
    "    \n",
    "    df.to_csv(FNAMES['crime-dataset'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def compute_distance(\n",
    "        lat_1, lon_1,\n",
    "        lat_2, lon_2):\n",
    "    \"\"\"Compute distance between two locations.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Distance in KM.\n",
    "    \n",
    "    Source: <https://stackoverflow.com/questions/19412462/>\n",
    "    \"\"\"\n",
    "    # Approximate radius of earth in KM\n",
    "    earth_radius = 6373.0\n",
    "    \n",
    "    lat_1 = np.radians(lat_1)\n",
    "    lon_1 = np.radians(lon_1)\n",
    "    \n",
    "    lat_2 = np.radians(lat_2)\n",
    "    lon_2 = np.radians(lon_2)\n",
    "    \n",
    "    lon_dist = lon_2 - lon_1\n",
    "    lat_dist = lat_2 - lat_1\n",
    "    \n",
    "    a = (np.square(np.sin(lat_dist/2))\n",
    "         + np.cos(lat_1)\n",
    "         * np.cos(lat_2)\n",
    "         * np.square(np.sin(lon_dist/2)))\n",
    "    \n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    \n",
    "    return earth_radius * c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def compute_aggregated_features(data, time_window, crime_radius):\n",
    "    \"\"\"Compute aggregated features.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray, dtype('int64')\n",
    "        A Numpy-like array of shape \"(n, m)\", where \"n\" is the number\n",
    "        of records and \"m\" is the number of columns (or attributes).\n",
    "        The strict order of the columns is presented below:\n",
    "            Dataset,\n",
    "            Id,\n",
    "            Dates,\n",
    "            PdDistrict,\n",
    "            X - Longitude,\n",
    "            Y - Latitude\n",
    "    time_window : int\n",
    "        Time window (in hours).\n",
    "    crime_radius : list\n",
    "        List of integers, each of which representing a radius in kilometers.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    \n",
    "    # Let's transform the time window into seconds\n",
    "    time_window = time_window * 60 * 60\n",
    "    \n",
    "    aggregated_features = []\n",
    "    for i in range(n):\n",
    "        ts = data[i,2]    \n",
    "        \n",
    "        lower_ts = ts - time_window\n",
    "        \n",
    "        mask = ((lower_ts < data[:,2])\n",
    "                & (data[:,2] < ts))\n",
    "        \n",
    "        historical_data = data[mask]\n",
    "        m = len(historical_data)\n",
    "        \n",
    "        police_district = data[i,3]\n",
    "        \n",
    "        feature_vector = [\n",
    "            int(data[i,0]),\n",
    "            int(data[i,1]),\n",
    "            m, # number of crimes within the time window\n",
    "            0 # number of crimes attended by the same police department district\n",
    "            ]\n",
    "        feature_vector = feature_vector + [0 for j in crime_radius]\n",
    "        \n",
    "        lat_1 = data[i,5]\n",
    "        lon_1 = data[i,4]\n",
    "        \n",
    "        for j in range(m):\n",
    "            feature_vector[3] += 1 if police_district == historical_data[j,3] else 0\n",
    "            \n",
    "            lat_2 = historical_data[j,5]\n",
    "            lon_2 = historical_data[j,4]\n",
    "            \n",
    "            # Let's compute the number of crimes within each given radius\n",
    "            distance = compute_distance(lat_1, lon_1, lat_2, lon_2)\n",
    "            \n",
    "            for k, rad in enumerate(crime_radius):\n",
    "                feature_vector[4+k] += 1 if distance <= rad else 0\n",
    "                \n",
    "        aggregated_features.append(feature_vector)\n",
    "        \n",
    "    return aggregated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_date_attributes(df, column, drop_column=True):\n",
    "    df['Year'] = df[column].dt.year\n",
    "    \n",
    "    df['Month'] = df[column].dt.month\n",
    "    df['Quarter'] = df[column].dt.quarter\n",
    "    \n",
    "    df['Triannual'] = 0\n",
    "    for i, (min_m, max_m) in enumerate([[1, 4], [5, 8], [9, 12]]):\n",
    "        df.loc[((min_m <= df['Month']) & (df['Month'] <= max_m)), 'Triannual'] = i + 1\n",
    "    \n",
    "    df['Semester'] = 1\n",
    "    df.loc[(df['Quarter'] > 2), 'Semester'] = 2\n",
    "    \n",
    "    df['Day'] = df[column].dt.day\n",
    "    df['DayOfWeek'] = df[column].dt.dayofweek\n",
    "    \n",
    "    df['WorkingDay'] = 1\n",
    "    df.loc[(df['DayOfWeek'] > 4), 'WorkingDay'] = 0\n",
    "    \n",
    "    df['Fortnight'] = 1\n",
    "    df.loc[(df['Day'] > 15), 'Fortnight'] = 2\n",
    "    \n",
    "    df['Hour'] = df[column].dt.hour\n",
    "    \n",
    "    hourly_periods = {\n",
    "        'four': [[i, i+3] for i in range(0, 24, 4)],\n",
    "        'six': [[i, i+5] for i in range(0, 24, 6)],\n",
    "        'twelve': [[i, i+11] for i in range(0, 24, 12)],\n",
    "        }\n",
    "    \n",
    "    for str_period, period in hourly_periods.items():\n",
    "        period_column = '{}HourPeriod'.format(str_period.title())\n",
    "        df[period_column] = 0\n",
    "        \n",
    "        for i, (min_hr, max_hr) in enumerate(period):\n",
    "            df.loc[((min_hr <= df['Hour']) & (df['Hour'] <= max_hr)), period_column] = i + 1\n",
    "    \n",
    "    if drop_column:\n",
    "        df = df.drop(columns=[column])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(\n",
    "        df, data, time_windows,\n",
    "        idx_to_dataset, idx_to_district,\n",
    "        crime_radius=[1, 2, 4, 8, 16]):\n",
    "    \"\"\"Compute the process of feature engineering.\"\"\"\n",
    "    df = df[['Dataset', 'Id', 'PdDistrict', 'Dates']]\n",
    "    df = df.replace({'Dataset': idx_to_dataset, 'PdDistrict': idx_to_district})\n",
    "    \n",
    "    df = derive_date_attributes(df, 'Dates')\n",
    "    \n",
    "    crime_radius = np.array(crime_radius, dtype=int)\n",
    "    \n",
    "    agg_ds_fname = os.path.join(DATA_PATH, 'agg-dataset-{}H.csv')\n",
    "    cum_ds_fname = os.path.join(DATA_PATH, 'agg-dataset-{}H-cumulative.csv')    \n",
    "    \n",
    "    for i, time_window in enumerate(time_windows):\n",
    "        agg_ds_fname_1 = agg_ds_fname.format(time_window)\n",
    "        cum_ds_fname_1 = cum_ds_fname.format(time_window)\n",
    "        \n",
    "        if (os.path.isfile(cum_ds_fname_1)\n",
    "                or (i == 0 and os.path.isfile(agg_ds_fname_1))):\n",
    "            continue\n",
    "        elif os.path.isfile(agg_ds_fname_1):\n",
    "            agg_ds = pd.read_csv(agg_ds_fname_1)\n",
    "            \n",
    "            for col in agg_ds.columns:\n",
    "                if col in ['Dataset']:\n",
    "                    continue\n",
    "                \n",
    "                agg_ds[col] = pd.to_numeric(agg_ds[col])\n",
    "        else:\n",
    "            agg_ds = compute_aggregated_features(data, time_window, crime_radius)\n",
    "            \n",
    "            prefix = '{}H_'.format(time_window)\n",
    "            agg_ds_columns = (['Dataset', 'Id']\n",
    "                              + [(prefix + col) for col in ['Crimes', 'CrimesAttendedByPdDistrict']]\n",
    "                              + [(prefix + 'CrimesWithin{}KMRad'.format(rad)) for rad in crime_radius])\n",
    "            \n",
    "            agg_ds = pd.DataFrame(agg_ds, columns=agg_ds_columns)\n",
    "            agg_ds = agg_ds.astype({col: 'int32' for col in agg_ds_columns})\n",
    "            \n",
    "            agg_ds['Dataset'] = agg_ds['Dataset'].map(idx_to_dataset)\n",
    "            \n",
    "            agg_ds.to_csv(agg_ds_fname_1, index=False)\n",
    "        \n",
    "        if i == 0:\n",
    "            continue\n",
    "            \n",
    "        cum_ds = agg_ds.copy(deep=True)\n",
    "        agg_ds = None\n",
    "        \n",
    "        for j in range(i):\n",
    "            agg_ds_fname_2 = agg_ds_fname.format(time_windows[j])\n",
    "            \n",
    "            agg_ds = pd.read_csv(agg_ds_fname_2)\n",
    "            \n",
    "            for col in agg_ds.columns:\n",
    "                if col in ['Dataset']:\n",
    "                    continue\n",
    "                \n",
    "                agg_ds[col] = pd.to_numeric(agg_ds[col])\n",
    "                \n",
    "            cum_ds = pd.merge(agg_ds, cum_ds, on=['Dataset', 'Id'], how='inner')\n",
    "            \n",
    "        cum_ds = pd.merge(df, cum_ds, on=['Dataset', 'Id'], how='inner')\n",
    "        \n",
    "        cum_ds.to_csv(cum_ds_fname_1, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FNAMES['crime-dataset'])\n",
    "\n",
    "df['Dates'] = pd.to_datetime(df['Dates'], format=DATE_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TO_IDX = {dataset: i for i, dataset in enumerate(df['Dataset'].unique())}\n",
    "IDX_TO_DATASET = {i: dataset for dataset, i in DATASET_TO_IDX.items()}\n",
    "\n",
    "df['Dataset'] = df['Dataset'].map(DATASET_TO_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTRICT_TO_IDX = {district: i for i, district in enumerate(df['PdDistrict'].unique())}\n",
    "IDX_TO_DISTRICT = {i: district for district, i in DISTRICT_TO_IDX.items()}\n",
    "\n",
    "df['PdDistrict'] = df['PdDistrict'].map(DISTRICT_TO_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECTION = (['Dataset']\n",
    "              + [col for col in PROJECTION if col not in ['Category', 'DayOfWeek']])\n",
    "\n",
    "df = df[PROJECTION]\n",
    "df = df.sort_values(by=['Dates', 'Dataset', 'Id'])\n",
    "\n",
    "crimes = df.copy(deep=True)\n",
    "crimes['ts'] = crimes['Dates'].values.astype(np.int64) // 10 ** 9\n",
    "crimes = crimes[[('ts' if col == 'Dates' else col) for col in PROJECTION]].to_numpy().astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bear in mind that running the feature engineering process takes a long time; approximately 7.3 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_WINDOWS = [12, 24, 72, 168, 336]\n",
    "\n",
    "feature_engineering(df, crimes, TIME_WINDOWS, IDX_TO_DATASET, IDX_TO_DISTRICT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Crime Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Train-Test Split\n",
    "\n",
    "Let's split the whole dataset of crimes into training, validation and test datasets. Please recall that the original training and test datasets, as downloaded from Kaggle, were merged to run the feature engineering process. Lastly, the training dataset will be split into two folds: the first one, approximately 80% of the data, is intended to train prediction models; the second one is aimed at validating each prediction model on an independent dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SIZE = 0.2\n",
    "\n",
    "for time_window in TIME_WINDOWS:\n",
    "    in_fname = os.path.join(DATA_PATH, 'agg-dataset-{}H-cumulative.csv'.format(time_window))\n",
    "    if not os.path.isfile(in_fname):\n",
    "        continue\n",
    "    \n",
    "    window_path = {\n",
    "        '/': os.path.join(DATA_PATH, '{}H'.format(time_window))\n",
    "        }\n",
    "    window_path['data'] = os.path.join(window_path['/'], 'data')\n",
    "    \n",
    "    for path in window_path.values():\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "    \n",
    "    window_fnames = {\n",
    "        ds: os.path.join(window_path['data'], '{}.csv'.format(ds))\n",
    "        for ds in ['train', 'validation', 'test']\n",
    "        }\n",
    "    \n",
    "    split_data = False\n",
    "    for fname in window_fnames.values():\n",
    "        if not os.path.isfile(fname):\n",
    "            split_data = True\n",
    "            break\n",
    "            \n",
    "    if not split_data:\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_csv(in_fname)    \n",
    "    \n",
    "    df['Id'] = pd.to_numeric(df['Id'])\n",
    "    \n",
    "    columns = df.columns    \n",
    "    \n",
    "    numerical_columns = [col for col in columns if re.match(r'[0-9]{2,3}H_', col)]    \n",
    "    \n",
    "    categorical_columns = ['PdDistrict']\n",
    "    for col in columns:\n",
    "        if (col not in numerical_columns\n",
    "                and col not in ['Dataset', 'Id', 'PdDistrict']):\n",
    "            categorical_columns.append(col)\n",
    "    \n",
    "    if not os.path.isfile(window_fnames['test']):\n",
    "        test = df.loc[df['Dataset']=='test'].drop(columns=['Dataset'])\n",
    "        \n",
    "        test = test.sort_values(by=['Id'])\n",
    "        \n",
    "        columns = ['Id'] + categorical_columns + numerical_columns\n",
    "        test = test[columns]\n",
    "        \n",
    "        test.to_csv(window_fnames['test'], index=False)\n",
    "        test = None\n",
    "        \n",
    "    df = df.loc[df['Dataset']=='train'].drop(columns=['Dataset'])\n",
    "    \n",
    "    train = pd.read_csv(FNAMES['train'])    \n",
    "    \n",
    "    train['Id'] = pd.to_numeric(train['Id'])\n",
    "    train = train[['Id', 'Category']]\n",
    "    \n",
    "    assert len(df) == len(train)\n",
    "    \n",
    "    df = pd.merge(df, train, on=['Id'], how='inner')\n",
    "    train = None\n",
    "    \n",
    "    columns = ['Id'] + categorical_columns + numerical_columns + ['Category']\n",
    "    df = df[columns]\n",
    "    \n",
    "    # The validation dataset is a stratified random sample\n",
    "    \n",
    "    validation = None\n",
    "    for category in CRIME_CATEGORY.keys():\n",
    "        category_samples = df.loc[df['Category']==category]\n",
    "        \n",
    "        sample_size = len(category_samples) * VALIDATION_SIZE\n",
    "        sample_size = int(np.round(sample_size, 0))\n",
    "        \n",
    "        sample = category_samples.sample(n=sample_size, replace=False, random_state=RANDOM_STATE)\n",
    "        \n",
    "        validation = (\n",
    "            validation.append(sample).reset_index(drop=True)\n",
    "            if validation is not None\n",
    "            else sample.copy(deep=True)\n",
    "            )\n",
    "    \n",
    "    validation.to_csv(window_fnames['validation'], index=False)\n",
    "    \n",
    "    df = df.loc[~df['Id'].isin(validation['Id'].values)]\n",
    "    df.to_csv(window_fnames['train'], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Baseline Model\n",
    "\n",
    "Let's build a strong baseline model according to the following criteria:\n",
    "\n",
    "1. The training dataset will be used to learn prediction models.\n",
    "2. The validation dataset will be used to rank prediction models, as they haven't seen these data during their training process.\n",
    "3. The entire set of features will be used.\n",
    "4. A set of several machine learning algorithms will be used to learn prediction models. Thus, a prediction model will be learned per each combination of algorithm and time window.\n",
    "5. There will not be hyperparameter optimization. Machine learning algorithms will be used with their default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_attributes(\n",
    "        df, attributes, time_window, window_type,\n",
    "        attributes_to_exclude=['Id', 'PdDistrict', 'Category']\n",
    "        ):\n",
    "    \"\"\"Identify the attribute names the sets of numerical and categorical attributes consist of.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of attribute names the set of numerical features consists of.\n",
    "    list\n",
    "        List of attribute names the set of categorical features consists of.\n",
    "    \"\"\"\n",
    "    columns = df.columns\n",
    "    \n",
    "    num_attr_re = re.compile(\n",
    "        r'{}H_'.format('[0-9]{2,3}' if window_type == 'cumulative' else time_window)\n",
    "        )\n",
    "    num_attr = (\n",
    "        [col for col in columns if num_attr_re.match(col)]\n",
    "        if attributes in ['num', 'all']\n",
    "        else []\n",
    "        )\n",
    "    \n",
    "    cat_attr = []\n",
    "    for col in columns:\n",
    "        if attributes == 'num':\n",
    "            break\n",
    "        \n",
    "        if (col not in attributes_to_exclude\n",
    "                and not re.match(r'[0-9]{2,3}H_', col)):\n",
    "            cat_attr.append(col)\n",
    "            \n",
    "    return num_attr, cat_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "        train, validation,\n",
    "        attributes, y_column,\n",
    "        time_window, window_type,\n",
    "        estimator, param_grid,\n",
    "        predict_proba,\n",
    "        return_pred=False\n",
    "        ):\n",
    "    \"\"\"Build a prediction model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train : pd.DataFrame\n",
    "    \n",
    "    validation : pd.DataFrame\n",
    "    \n",
    "    attributes : str\n",
    "        The set of attributes to be used as input by the estimator.\n",
    "        \n",
    "        - If \"cat\", then the date-derived attributes are used.\n",
    "        - If \"num\", then the aggregated attributes computed using the time window are used.\n",
    "        - If \"all\", then the union of the attributes \"cat\" and \"raw\" is used.\n",
    "        \n",
    "        Bear in mind that, whatever the set of attributes,\n",
    "        the attribute \"PdDistrict\" is always used.\n",
    "    \n",
    "    y_column : str\n",
    "        Which is the target variable? This variable must\n",
    "        be in both training and validation datasets.\n",
    "    \n",
    "    time_window : int\n",
    "        Time window (in hours).\n",
    "    \n",
    "    window_type : str\n",
    "        Whether to use other windows whose time in hours is less than the specified time window value.\n",
    "        \n",
    "        - If \"exact\", then aggregated attributes that were computed using only the specified time\n",
    "          window, are used.\n",
    "        - If \"cumulative\", then aggregated attributes that were computed using windows whose time\n",
    "          in hours is less than the specified time window, are also used.\n",
    "    \n",
    "    estimator\n",
    "        A scikit-learn classification estimator.\n",
    "    \n",
    "    param_grid : dict\n",
    "        Dictionary of parameters, as well as their corresponding values, for the estimator.\n",
    "        This enables an exhaustive search over the specified parameter values through cross-validation.\n",
    "        \n",
    "    predict_proba : bool\n",
    "        Whether or not the estimator supports the \"predict_proba()\" method.\n",
    "    \n",
    "    return_pred : bool\n",
    "    \"\"\"\n",
    "    num_attr, cat_attr = identify_attributes(train, attributes, time_window, window_type)\n",
    "    \n",
    "    if 'PdDistrict' in train.columns:\n",
    "        cat_attr.insert(0, 'PdDistrict')\n",
    "    \n",
    "    X_train_cat = train[cat_attr].to_numpy()\n",
    "    X_valid_cat = validation[cat_attr].to_numpy()\n",
    "    \n",
    "    X_train_num = None\n",
    "    X_valid_num = None\n",
    "    \n",
    "    if attributes in ['num', 'all']:\n",
    "        scaler = StandardScaler()\n",
    "        X_train_num = scaler.fit_transform(train[num_attr].to_numpy().astype(float))\n",
    "        \n",
    "        X_valid_num = scaler.transform(validation[num_attr].to_numpy().astype(float))\n",
    "    \n",
    "    X_train = (\n",
    "        np.hstack([X_train_cat, X_train_num])\n",
    "        if X_train_num is not None\n",
    "        else X_train_cat\n",
    "        )\n",
    "    \n",
    "    X_valid = (\n",
    "        np.hstack([X_valid_cat, X_valid_num])\n",
    "        if X_valid_num is not None\n",
    "        else X_valid_cat\n",
    "        )\n",
    "    \n",
    "    y_train = train[y_column].values.astype(int)\n",
    "    y_valid = validation[y_column].values.astype(int)\n",
    "    \n",
    "    clf = estimator\n",
    "    \n",
    "    if isinstance(param_grid, dict):\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "        \n",
    "        scoring = 'neg_log_loss' if predict_proba else 'f1_macro'\n",
    "        clf = GridSearchCV(\n",
    "            estimator=estimator, param_grid=param_grid, cv=cv,\n",
    "            n_jobs=6, scoring=scoring, iid=False, refit=True\n",
    "            )\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict_proba(X_valid) if predict_proba else clf.predict(X_valid)    \n",
    "    \n",
    "    score = log_loss(y_valid, y_pred)\n",
    "    \n",
    "    if not return_pred:\n",
    "        return score\n",
    "    \n",
    "    return score, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_baseline_model(\n",
    "        time_windows, out_fname,\n",
    "        district_to_idx=DISTRICT_TO_IDX,\n",
    "        crime_category=CRIME_CATEGORY,\n",
    "        algorithms=ALGORITHMS):\n",
    "    \"\"\"Find the best baseline model, as specified above.\"\"\"\n",
    "    results = []\n",
    "    results_header = ['TimeWindow', 'Algorithm', 'LogLoss']\n",
    "    \n",
    "    for time_window in time_windows:\n",
    "        window_path = {\n",
    "            '/': os.path.join(DATA_PATH, '{}H'.format(time_window))\n",
    "            }\n",
    "    \n",
    "        window_path['data'] = os.path.join(window_path['/'], 'data')\n",
    "        \n",
    "        train = pd.read_csv(os.path.join(window_path['data'], 'train.csv'))\n",
    "        train = train.replace({'PdDistrict': district_to_idx, 'Category': crime_category})\n",
    "        \n",
    "        validation = pd.read_csv(os.path.join(window_path['data'], 'validation.csv'))\n",
    "        validation = validation.replace({'PdDistrict': district_to_idx, 'Category': crime_category})\n",
    "        \n",
    "        for algo, settings in algorithms.items():\n",
    "            score = build_model(\n",
    "                train, validation,\n",
    "                'all', 'Category',\n",
    "                time_window, 'cumulative',\n",
    "                settings['estimator'], None,\n",
    "                settings['predict_proba']\n",
    "                )\n",
    "            results.append([str(time_window), algo, '{:.5f}'.format(score)])\n",
    "            \n",
    "            pd.DataFrame(results, columns=results_header).to_csv(out_fname, index=False, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding a strong baseline model takes a long time. Therefore, the set of time windows will be reduced, namely: 24, 72, and 168 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_WINDOWS.pop(0)\n",
    "\n",
    "if not os.path.isfile(FNAMES['baseline']):\n",
    "    find_baseline_model(TIME_WINDOWS[:-1], FNAMES['baseline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = pd.read_csv(FNAMES['baseline'])\n",
    "\n",
    "for col in baseline_results.columns:\n",
    "    if col in ['Algorithm']:\n",
    "        continue\n",
    "    \n",
    "    baseline_results[col] = pd.to_numeric(baseline_results[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = baseline_results.pivot(index='TimeWindow', columns='Algorithm', values='LogLoss')\n",
    "\n",
    "print(chart_data)\n",
    "\n",
    "chart = chart_data.plot(kind='bar', grid=True, legend=True, x=None)\n",
    "chart.set_xlabel('Time window')\n",
    "chart.set_ylabel('Log loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably, the Logistic Regression algorithm outperforms all of the machine learning algorithms, even though the best result, i.e., **2.58249**, was achieved by the Gradient Boosting algorithm on data aggregated using a time window of 72 hours. This conclusion is drawn as Logistic Regression is the most computationally efficient algorithm, and the difference between its best result and the overall best result is negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, let's analyze how having a larger time window contributes to crime classification. The Logistic Regression algorithm will be used to perform this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNAMES['logit-baseline'] = os.path.join(DATA_PATH, 'baseline-logit-models.csv')\n",
    "\n",
    "if not os.path.isfile(FNAMES['logit-baseline']):\n",
    "    logit_baseline = baseline_results.loc[baseline_results['Algorithm']=='logit']\n",
    "    logit_baseline = logit_baseline.drop(columns=['Algorithm'])\n",
    "    \n",
    "    time_window = 336\n",
    "    \n",
    "    window_data_path = os.path.join(DATA_PATH, '{}H'.format(time_window), 'data')\n",
    "    \n",
    "    train = pd.read_csv(os.path.join(window_data_path, 'train.csv'))\n",
    "    train = train.replace({'PdDistrict': DISTRICT_TO_IDX, 'Category': CRIME_CATEGORY})\n",
    "    \n",
    "    validation = pd.read_csv(os.path.join(window_path['data'], 'validation.csv'))\n",
    "    validation = validation.replace({'PdDistrict': DISTRICT_TO_IDX, 'Category': CRIME_CATEGORY})\n",
    "    \n",
    "    algo = ALGORITHMS['logit']\n",
    "    \n",
    "    score = build_model(\n",
    "        train, validation,\n",
    "        'all', 'Category',\n",
    "        time_window, 'cumulative',\n",
    "        algo['estimator'], None,\n",
    "        algo['predict_proba']\n",
    "        )\n",
    "    \n",
    "    logit_baseline = logit_baseline.append({'TimeWindow': time_window, 'LogLoss': score}, ignore_index=True)\n",
    "    \n",
    "    logit_baseline.to_csv(FNAMES['logit-baseline'], index=False, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_baseline = pd.read_csv(FNAMES['logit-baseline'])\n",
    "\n",
    "logit_baseline['LogLoss'] = pd.to_numeric(logit_baseline['LogLoss'])\n",
    "\n",
    "print(logit_baseline)\n",
    "\n",
    "chart = logit_baseline.plot(x='TimeWindow', y='LogLoss', kind='bar', grid=True, legend=False)\n",
    "chart.set_xlabel('Time window')\n",
    "chart.set_ylabel('Log loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, the **baseline result** is **2.5947**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Discriminative Features\n",
    "\n",
    "The goal is to quantify how discriminative each set of features is, as well as each window type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminative_feature(\n",
    "        time_window, algorithm, out_fname,\n",
    "        district_to_idx=DISTRICT_TO_IDX,\n",
    "        crime_category=CRIME_CATEGORY):\n",
    "    \"\"\"Quantify how discriminative each set of features is, as well as each window type.\"\"\"\n",
    "    window_data_path = os.path.join(DATA_PATH, '{}H'.format(time_window), 'data')\n",
    "    \n",
    "    train = pd.read_csv(os.path.join(window_data_path, 'train.csv'))\n",
    "    train = train.replace({'PdDistrict': district_to_idx, 'Category': crime_category})\n",
    "    \n",
    "    validation = pd.read_csv(os.path.join(window_data_path, 'validation.csv'))\n",
    "    validation = validation.replace({'PdDistrict': district_to_idx, 'Category': crime_category})\n",
    "    \n",
    "    grid = {\n",
    "        'attributes': ['all', 'num', 'cat'],\n",
    "        'window_type': ['exact', 'cumulative']\n",
    "        }\n",
    "    \n",
    "    results = []\n",
    "    results_header = ['Attributes', 'WindowType', 'LogLoss']\n",
    "    \n",
    "    for params in ParameterGrid(grid):\n",
    "        score = build_model(\n",
    "            train, validation,\n",
    "            params['attributes'], 'Category',\n",
    "            time_window, params['window_type'],\n",
    "            algorithm['estimator'], None,\n",
    "            algorithm['predict_proba']\n",
    "            )\n",
    "        \n",
    "        results.append([params['attributes'], params['window_type'], '{:.5f}'.format(score)])\n",
    "        \n",
    "        pd.DataFrame(results, columns=results_header).to_csv(out_fname, index=False, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNAMES['discriminative-features'] = os.path.join(DATA_PATH, 'discriminative-features.csv')\n",
    "\n",
    "if not os.path.isfile(FNAMES['discriminative-features']):\n",
    "    discriminative_feature(\n",
    "        336, ALGORITHMS['logit'], FNAMES['discriminative-features']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminative_features = pd.read_csv(FNAMES['discriminative-features'])\n",
    "\n",
    "discriminative_features['LogLoss'] = pd.to_numeric(discriminative_features['LogLoss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_data = discriminative_features.pivot(index='Attributes', columns='WindowType', values='LogLoss')\n",
    "\n",
    "print(chart_data)\n",
    "\n",
    "chart = chart_data.plot(kind='bar', grid=True, legend=True)\n",
    "\n",
    "chart.legend(title='Window type', bbox_to_anchor=(1.0, 1.0))\n",
    "\n",
    "chart.set_xlabel('Attributes')\n",
    "chart.set_ylabel('Log loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results show that the union of the numerical and categorical attributes contribute the most to discriminative power. In a like manner, aggregated attributes computed using cumulative time windows rather than an exact time window are more discriminative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Ensemble Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_in_file(fname, content, mode='w', insert_new_line=True):\n",
    "    with open(fname, mode) as f:\n",
    "        f.write(content + ('\\n' if insert_new_line else ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def is_leap_year(year):\n",
    "    return (True\n",
    "            if (year % 4 == 0 and (year % 100 != 0 or year % 400 == 0))\n",
    "            else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def get_number_of_days_in_month(data):\n",
    "    n = len(data)\n",
    "    \n",
    "    days_in_month = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "    \n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        year = data[i,0]\n",
    "        month = data[i,1]\n",
    "        \n",
    "        result.append(\n",
    "            (29 if is_leap_year(year) else 28)\n",
    "            if month == 2\n",
    "            else days_in_month[month-1]\n",
    "            )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cyclical_attributes(\n",
    "        df,\n",
    "        attributes=['Month', 'Day', 'DayOfWeek', 'Hour']):\n",
    "    \"\"\"Encoding of cyclical attributes.\n",
    "    \n",
    "    Source: <http://blog.davidkaleko.com/feature-engineering-cyclical-features.html>\n",
    "    \"\"\"\n",
    "    df['days_month'] = get_number_of_days_in_month(df[['Year', 'Month']].to_numpy().astype(int))\n",
    "    \n",
    "    for attr in attributes:\n",
    "        max_val = df[attr].max() if attr != 'Day' else df['days_month']\n",
    "        min_val = df[attr].min()\n",
    "        \n",
    "        if min_val == 1:\n",
    "            df[attr] -= 1\n",
    "        elif min_val == 0:\n",
    "            max_val += 1\n",
    "        \n",
    "        df['{}_Sin'.format(attr)] = np.sin(df[attr] * 2 * np.pi / max_val)\n",
    "        df['{}_Cos'.format(attr)] = np.cos(df[attr] * 2 * np.pi / max_val)\n",
    "        \n",
    "    attributes.append('days_month')\n",
    "    df = df.drop(columns=attributes)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_attributes(df, attributes):\n",
    "    \"\"\"One-Hot encoding of categorical attributes.\"\"\"\n",
    "    return pd.get_dummies(df, prefix=attributes, columns=attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_training_data(df, n_splits, split_criterion='Category'):\n",
    "    criterion_values = {\n",
    "        value: df.loc[df[split_criterion]==value].shape[0]\n",
    "        for value in df[split_criterion].unique()\n",
    "        }\n",
    "    \n",
    "    sampled_instances = []\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        last_split = True if i == (n_splits-1) else False\n",
    "        \n",
    "        sample = None\n",
    "        for value, size in criterion_values.items():\n",
    "            mask = (df[split_criterion] == value) & (~df['Id'].isin(sampled_instances))\n",
    "            criterion_sample = df.loc[mask]\n",
    "            \n",
    "            sample_size = size * (1/n_splits)\n",
    "            sample_size = int(np.round(sample_size, 0))\n",
    "            \n",
    "            if not last_split:\n",
    "                criterion_sample = criterion_sample.sample(\n",
    "                    n=sample_size, replace=False, random_state=RANDOM_STATE\n",
    "                    )\n",
    "                \n",
    "            sample = (\n",
    "                sample.append(criterion_sample).reset_index(drop=True)\n",
    "                if sample is not None\n",
    "                else criterion_sample.copy(deep=True)\n",
    "                )\n",
    "            \n",
    "            sampled_instances += criterion_sample['Id'].values.tolist()\n",
    "        \n",
    "        yield sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_ensembles(\n",
    "        algorithm=ALGORITHMS['logit'],\n",
    "        crime_category=CRIME_CATEGORY,\n",
    "        district_to_idx=DISTRICT_TO_IDX,\n",
    "        time_windows=TIME_WINDOWS):\n",
    "    \"\"\"Build classifier ensembles.\"\"\"\n",
    "    prediction_header = ['' for j in range(len(crime_category))]\n",
    "    for crime, idx in crime_category.items():\n",
    "        prediction_header[idx] = crime\n",
    "    \n",
    "    ensemble_grid = {\n",
    "        'encode_cyclical_attr': [True, False],\n",
    "        'encode_date_attr': [True, False],\n",
    "        'encode_district_attr': [True, False],\n",
    "        'n_estimators': [3, 5, 7],\n",
    "        'stack_method': ['hard_voting', 'soft_voting', 'clf'],\n",
    "        'time_window': time_windows\n",
    "        }\n",
    "    \n",
    "    for settings in ParameterGrid(ensemble_grid):\n",
    "        time_window = settings['time_window']\n",
    "        \n",
    "        window_path = {\n",
    "            '/': os.path.join(DATA_PATH, '{}H'.format(time_window))\n",
    "            }        \n",
    "        window_path['/data'] = os.path.join(window_path['/'], 'data')\n",
    "        \n",
    "        train = pd.read_csv(os.path.join(window_path['/data'], 'train.csv'))\n",
    "        validation = pd.read_csv(os.path.join(window_path['/data'], 'validation.csv'))\n",
    "        \n",
    "        if settings['encode_cyclical_attr']:\n",
    "            train = encode_cyclical_attributes(train)\n",
    "            validation = encode_cyclical_attributes(validation)\n",
    "        \n",
    "        if settings['encode_date_attr']:\n",
    "            date_attr = [\n",
    "                'Quarter', 'Triannual', 'Semester', 'Fortnight',\n",
    "                'FourHourPeriod', 'SixHourPeriod', 'TwelveHourPeriod'\n",
    "                ]\n",
    "            \n",
    "            train = encode_categorical_attributes(train, date_attr)\n",
    "            validation = encode_categorical_attributes(validation, date_attr)\n",
    "        \n",
    "        if settings['encode_district_attr']:\n",
    "            train = encode_categorical_attributes(train, ['PdDistrict'])\n",
    "            validation = encode_categorical_attributes(validation, ['PdDistrict'])\n",
    "        else:\n",
    "            train = train.replace({'PdDistrict': district_to_idx})\n",
    "            validation = validation.replace({'PdDistrict': district_to_idx})\n",
    "            \n",
    "        train = train.replace({'Category': crime_category})\n",
    "        validation = validation.replace({'Category': crime_category})\n",
    "        \n",
    "        window_path['/prediction'] = os.path.join(\n",
    "            window_path['/'],\n",
    "            'prediction',\n",
    "            'validation',\n",
    "            'stack_method={}'.format(settings['stack_method']),\n",
    "            'n_estimators={}'.format(settings['n_estimators']),\n",
    "            'cyclical_attr={};date_attr={};district_attr={}'.format(*[\n",
    "                ('True' if settings['encode_{}_attr'.format(var)] else 'False')\n",
    "                for var in ['cyclical', 'date', 'district']\n",
    "                ])\n",
    "            )\n",
    "        \n",
    "        if not os.path.isdir(window_path['/prediction']):\n",
    "            os.makedirs(window_path['/prediction'])\n",
    "            \n",
    "        learners_result_fname = os.path.join(window_path['/prediction'], 'learners-result.csv')\n",
    "        if not os.path.isfile(learners_result_fname):\n",
    "            write_in_file(learners_result_fname, ','.join(['Clf', 'LogLoss']))\n",
    "        \n",
    "        for i, train_sample in enumerate(split_training_data(train, settings['n_estimators'])):\n",
    "            pred_fname = os.path.join(window_path['/prediction'], 'clf-{}-pred.csv'.format(i))\n",
    "            if os.path.isfile(pred_fname):\n",
    "                continue\n",
    "            \n",
    "            score, predictions = build_model(\n",
    "                train_sample, validation,\n",
    "                'all', 'Category',\n",
    "                time_window, 'cumulative',\n",
    "                algorithm['estimator'], None,\n",
    "                algorithm['predict_proba'],\n",
    "                return_pred=True\n",
    "                )\n",
    "            \n",
    "            write_in_file(\n",
    "                learners_result_fname, ','.join([str(i), '{:.5f}'.format(score)]), mode='a'\n",
    "                )\n",
    "            \n",
    "            predictions = pd.DataFrame(predictions, columns=prediction_header)\n",
    "            \n",
    "            predictions['Id'] = validation['Id'].values\n",
    "            predictions = predictions[['Id']+prediction_header]\n",
    "            \n",
    "            predictions.to_csv(pred_fname, float_format='%.5f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_classifier_ensembles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
